{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PSYN Simon Coupermant et Anthony Lwin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Importation des bibliothèques nécessaires\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m  \u001b[38;5;66;03m# Manipulation de données\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m  \u001b[38;5;66;03m# Calculs numériques\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m  \u001b[38;5;66;03m# Visualisation de données\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd  # Manipulation de données\n",
    "import numpy as np  # Calculs numériques\n",
    "import matplotlib.pyplot as plt  # Visualisation de données\n",
    "import seaborn as sns  # Visualisation avancée\n",
    "\n",
    "# Importation des bibliothèques de scikit-learn pour la modélisation\n",
    "from sklearn.model_selection import train_test_split  # Division des données en ensembles d'entraînement et de test\n",
    "from sklearn.ensemble import RandomForestClassifier  # Modèle de classification Random Forest\n",
    "from sklearn.linear_model import LinearRegression  # Modèle de régression linéaire\n",
    "from sklearn.pipeline import make_pipeline  # Construction de pipelines pour le prétraitement et les modèles\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures  # Normalisation et transformation polynomiale\n",
    "from sklearn.decomposition import PCA  # Réduction de dimension avec PCA\n",
    "from sklearn.metrics import accuracy_score, classification_report   # Évaluation des modèles\n",
    "# Import des outils de scikit-learn nécessaires\n",
    "from sklearn.model_selection import GridSearchCV  # Optimisation des hyperparamètres\n",
    "from sklearn.metrics import roc_auc_score, roc_curve  # Évaluation avec AUC-ROC\n",
    "from sklearn.model_selection import cross_val_score  # Validation croisée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Acquisition des données et Exploration des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data path and folder\n",
    "path_phone = \"C:/Users/scoup/Downloads/arrow_data_psyn.csv\"\n",
    "df = pd.read_csv(path_phone, on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape # Dimension du jeu de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head() # Affichage des premières lignes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nInformations sur le DataFrame :\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nStatistiques descriptives :\")\n",
    "print(df.describe(include='all'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse exploratoire des données (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Préparation des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il y a un doublon de la valeur Maximum power dissipation(Mw), on va supprimer la deuxième colonne Maximum power dissipation car elle contient moins de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppression de la colonne \"Maximum power dissipation (MW\"\n",
    "column_to_remove = \"Maximum power dissipation (MW\"\n",
    "if column_to_remove in df.columns:\n",
    "    df = df.drop(columns=[column_to_remove])\n",
    "\n",
    "# Vérification des colonnes restantes\n",
    "print(\"Colonnes restantes dans le DataFrame :\", df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse des données manquantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data_rows = df[df.isnull().any(axis=1)]\n",
    "print(f\"% de données manquantes est : {(len(missing_data_rows)/len(df))*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyse des données manquantes pour les différentes colonnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = df.columns\n",
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentages = []\n",
    "for i in range(2, 15):\n",
    "   missing_col = df[df.iloc[:, i].isnull()]\n",
    "   percentage = round((len(missing_col)/len(df))*100, 2)\n",
    "   percentages.append(percentage)\n",
    "   print(f\"{col[i]} : {percentage:.2f}% de valeurs manquantes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentages[1] # Récupération du pourcentage de valeurs manquantes pour la colonne \"Type\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Affichage du pourcentage des données manquantes selon les différentes colonnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionnaire des pourcentages de valeurs manquantes\n",
    "data = { \n",
    "   'Fabricant': percentages[0],\n",
    "   'Type': percentages[1],\n",
    "   'Configuration': percentages[2],\n",
    "   'Nominal Zener Voltage (V)': percentages[3],\n",
    "   'Zener voltage tolerance': percentages[4],\n",
    "   'Maximum power dissipation (MW)': percentages[5],\n",
    "   'Test Current (MA)': percentages[6],\n",
    "   'Maximum reverse leaking current (UA)': percentages[7],\n",
    "   'Maximum regulator current (MA)': percentages[8],\n",
    "   'Maximum Zener impedance (OHM)': percentages[9],\n",
    "   'Packaging': percentages[10],\n",
    "   'Pin Count': percentages[11],\n",
    "   'SVHC': percentages[12]\n",
    "}\n",
    "\n",
    "# Trier les données en ordre décroissant en fonction des valeurs\n",
    "sorted_data = dict(sorted(data.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "# Création du graphique en barres VERTICALES (bar au lieu de barh)\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(list(sorted_data.keys()), list(sorted_data.values()))  # Utilisation de plt.bar pour vertical\n",
    "\n",
    "# Ajout des labels et du titre\n",
    "plt.ylabel('Pourcentage de valeurs manquantes')\n",
    "plt.xlabel('Colonnes')\n",
    "plt.title('Pourcentage de valeurs manquantes par colonne (trié décroissant)')\n",
    "\n",
    "# Rotation des étiquettes pour une meilleure lisibilité\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Ajustement automatique de la disposition\n",
    "plt.tight_layout()\n",
    "\n",
    "# Affichage du graphique\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les colonnes Maximum power dissipation (MW) (un des inputs) et SVHC(la colonne target) sont à étudier car ces deux colonnes sont peu remplies (respectivement 72% et 45% de données manquantes) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nettoyage et Prétaitement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_svhc = df[df['SVHC'] == 1] # Filtrage des données avec SVHC = 1 (présence de substances dangereuses)\n",
    "df_Nan =  df[df['SVHC'].isna()] # Filtrage des données avec SVHC = NaN (données manquantes)\n",
    "df_nvhc = df[df['SVHC'] == 0] # Filtrage des données avec SVHC = 0 (absence de substances dangereuses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_svhc.shape # Dimension du jeu de données avec SVHC = 1  (présence de substances dangereuses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nvhc.shape # Dimension du jeu de données avec SVHC = 0 (absence de substances dangereuses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Nan.shape # Dimension du jeu de données avec SVHC = NaN (données manquantes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.concat([df_svhc, df_nvhc]) # Concaténation des données avec SVHC = 1 et SVHC = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head() # Affichage des premières lignes du jeu de données d'entraînement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La référence Zener étant uniquement une description de certaines caractéristiques de la diode, la colonne Réference n'est pas utile pour la prédiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_ss_ref = df_train.drop(columns=['Reference ZENER']) # Suppression de la colonne \"Reference ZENER\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modification des colonnes ayant des données mixtes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pour analyser nos données et faire de la prédictions, nous avons besoin d'avoir tout nos inputs de forme numérique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans la colonne \"Maximum regulator current (MA)\" on a des valeurs de ce type 56@Ta=50C ce qui signifique que le Maximum regulator current est 56 Ma pour une température ambiante de  50°C. On doit prendre que le Ma. Mais on va supprimer la colonne \"Maximum regulator current (MA)\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour extraire la valeur avant '@' ou laisser la valeur d'origine si '@' n'est pas présent\n",
    "def extract_current_before_at(value):\n",
    "    if isinstance(value, str) and \"@\" in value:\n",
    "        return float(value.split('@')[0])  # Extrait la partie avant '@' comme un float\n",
    "    return value  # Retourne la valeur d'origine (par ex. : 9.2)\n",
    "\n",
    "# Fonction pour extraire la température après '@'\n",
    "def extract_temperature_after_at(value):\n",
    "    if isinstance(value, str) and \"@\" in value:\n",
    "        temp_part = value.split('@')[1].strip()  # Extrait la partie après '@'\n",
    "        if \"=\" in temp_part and \"C\" in temp_part:\n",
    "            try:\n",
    "                return float(temp_part.split('=')[1].replace('C', '').strip())  # Extrait la température entre '=' et 'C'\n",
    "            except ValueError:\n",
    "                return None  # Retourne None en cas d'erreur\n",
    "    return None  # Retourne None si '@' n'est pas présent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "# Cas de test\n",
    "print(extract_temperature_after_at(\"83@Ta=50C\"))  # Résultat attendu : 50\n",
    "print(extract_temperature_after_at(\"162@Ta=25C\"))  # Résultat attendu : 25\n",
    "print(extract_temperature_after_at(\"83 @Ta=50C\"))  # Résultat attendu : 50 (avec un espace avant @)\n",
    "print(extract_current_before_at(\"50\"))  # Résultat attendu : 50 (avec un espace après @)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de la colonne pour la température ambiante avant de modifier la colonne originale\n",
    "df_train_ss_ref[\"Maximum Regulator Current Ambient Temperature (°C)\"] = df_train_ss_ref[\"Maximum regulator current (MA)\"].apply(extract_temperature_after_at)\n",
    "\n",
    "# Mise à jour de la colonne \"Maximum regulator current (MA)\" pour ne garder que les valeurs avant '@'\n",
    "df_train_ss_ref[\"Maximum regulator current (MA)\"] = df_train_ss_ref[\"Maximum regulator current (MA)\"].apply(extract_current_before_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Réorganiser les colonnes pour placer \"Maximum Regulator Current Ambient Temperature (°C)\" après \"Maximum regulator current (MA)\"\n",
    "columns = list(df_train_ss_ref.columns)\n",
    "index = columns.index(\"Maximum regulator current (MA)\")\n",
    "columns.insert(index + 1, columns.pop(columns.index(\"Maximum Regulator Current Ambient Temperature (°C)\")))\n",
    "df_train_ss_ref = df_train_ss_ref[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_ss_ref.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans la colonne colonne Maximum Zener impedance (OHM), on va enlever les TYP sur les données. Des données de la forme 50(Typ) vont devenir 50. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extraire les valeurs avant Typ dans la colonne  Maximum Zener impedance (OHM)\n",
    "def extract_current_before_typ(value):\n",
    "    if isinstance(value, str) and \"(Typ)\" in value:\n",
    "        return float(value.split('(Typ)')[0])  # Extrait la partie avant '(Typ)' comme un float\n",
    "    return value  # Retourne la valeur d'origine (par ex. : 9.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test \n",
    "print(extract_current_before_typ(\"50(Typ)\"))  # Résultat attendu : 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_ss_ref[\"Maximum Zener impedance (OHM)\"] = df_train_ss_ref[\"Maximum Zener impedance (OHM)\"].apply(extract_current_before_typ)\n",
    "\n",
    "df_train_ss_ref[\"Maximum Zener impedance (OHM)\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va transformer les % de la colonne Zener voltage tolerance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_ss_ref['Zener volatge tolerance'] = df_train_ss_ref['Zener volatge tolerance'].str.replace('%', '').astype(float) / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_ss_ref['Zener volatge tolerance'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer les données catégorielles en données numériques de Maximum regulator current (MA) et de  Maximum Zener impedance (OHM)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tout d'abord nous allons Transformer les données catégorielles en données numériques de Maximum regulator current (MA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_ss_ref['Maximum regulator current (MA)'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_ss_ref['Maximum regulator current (MA)'] = pd.to_numeric(\n",
    "    df_train_ss_ref['Maximum regulator current (MA)'], errors='coerce'\n",
    ")\n",
    "df_train_ss_ref['Maximum regulator current (MA)'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant nous allons transformer les données catégorielles en données numériques de Maximum Zener impedance (OHM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_ss_ref['Maximum Zener impedance (OHM)'] = pd.to_numeric(\n",
    "    df_train_ss_ref['Maximum Zener impedance (OHM)'], errors='coerce'\n",
    ")\n",
    "df_train_ss_ref['Maximum Zener impedance (OHM)'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va créer un nouveau csv pour vérifier que les données ont bien été traitées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder le fichier mis à jour\n",
    "updated_file_path_2 = 'C:/Users/scoup/Downloads/updated_arrow_data_lv_with_temperature.csv'\n",
    "df_train_ss_ref.to_csv(updated_file_path_2, index=False)\n",
    "updated_file_path_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_ss_ref.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_ss_ref[\"Maximum Zener impedance (OHM)\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train_ss_ref.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA pour voir les principales composantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Peut être encoder de manière numérique les colonnes Type et Configuration  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train_ss_ref.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Supposons que les colonnes sont nommées \"a\", \"b\", ..., \"p\"\n",
    "columns = list(df_train_ss_ref.columns)\n",
    "\n",
    "# Sélection des colonnes de la 5e à l'avant-dernière, sans prendre la 3e et 4e\n",
    "selected_columns = columns[1:2] + columns[4:-1]  # Exclut la 3e et 4e colonnes\n",
    "print(\"Colonnes sélectionnées pour la PCA :\", selected_columns)\n",
    "\n",
    "# Filtrer le DataFrame pour ne garder que les colonnes sélectionnées\n",
    "df_selected = df_train_ss_ref[selected_columns]\n",
    "\n",
    "len(df_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supprimer les lignes contenant des NaN dans les colonnes sélectionnées\n",
    "df_selected_ss_na = df_selected.dropna()\n",
    "\n",
    "# Standardiser les données (nécessaire pour la PCA)\n",
    "df_selected_ss_na_scaled = StandardScaler().fit_transform(df_selected_ss_na)\n",
    "\n",
    "# Effectuer la PCA\n",
    "pca = PCA(n_components=2)  # Réduire à 2 dimensions principales\n",
    "principalComponents = pca.fit_transform(df_selected_ss_na_scaled)\n",
    "\n",
    "# Créer un DataFrame avec les composantes principales\n",
    "principalDf = pd.DataFrame(data=principalComponents, columns=['principal component 1', 'principal component 2'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajouter la colonne target pour les indices correspondants après dropna()\n",
    "print(principalDf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenir les charges (importance des colonnes dans les composantes principales)\n",
    "loadings = pd.DataFrame(\n",
    "    pca.components_.T,\n",
    "    columns=['Principal Component 1', 'Principal Component 2'],\n",
    "    index=selected_columns\n",
    ")\n",
    "\n",
    "print(\"Charges des colonnes dans les composantes principales :\")\n",
    "print(loadings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifier les colonnes les plus importantes pour la première composante principale\n",
    "important_columns_pc1 = loadings['Principal Component 1'].abs().sort_values(ascending=False)\n",
    "print(\"Colonnes les plus importantes pour la première composante principale :\")\n",
    "print(important_columns_pc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifier les colonnes les plus importantes pour la première composante principale\n",
    "important_columns_pc2 = loadings['Principal Component 2'].abs().sort_values(ascending=False)\n",
    "print(\"Colonnes les plus importantes pour la première composante principale :\")\n",
    "print(important_columns_pc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Variance expliquée par chaque composante :\", pca.explained_variance_ratio_)\n",
    "print(\"Variance totale expliquée :\", sum(pca.explained_variance_ratio_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Les colonnes Fabricant, Packaging et Pin Count ne sont pas importantes car elles ont une variance de 0 dans PC1 et PC2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected_ss_na\n",
    "len(df_selected_ss_na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colonnes à conserver\n",
    "important_columns = [\n",
    "    \"Maximum power dissipation (MW)\",\n",
    "    \"Maximum Regulator Current Ambient Temperature (°C)\",\n",
    "    \"Test Current (MA)\",\n",
    "    \"Maximum regulator current (MA)\",\n",
    "    \"Maximum reverse leaking current (UA)\",\n",
    "    \"0minal Zener Voltage (V)\",\n",
    "    \"Maximum Zener impedance (OHM)\"\n",
    "]\n",
    "\n",
    "# Filtrer le DataFrame et créer une copie explicite\n",
    "df_final = df_train_ss_ref[important_columns].copy()\n",
    "#Ajout de la colonne SVHC target \n",
    "df_final['SVHC'] = df_train_ss_ref['SVHC']\n",
    "# Vérification des colonnes restantes\n",
    "print(\"Colonnes restantes après réduction :\", df_final.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Supposons que 'SVHC' soit la variable cible\n",
    "X = df_final[important_columns]\n",
    "y = df_final['SVHC']\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest simple "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer le modèle de Random Forest\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Entraîner le modèle\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Prédire sur l'ensemble de test\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Évaluer le modèle\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Classification Report:\\n{report}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amélioration du random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Étape 1 : Création du modèle de Random Forest (avec hyperparamètres de base)\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Étape 2 : Validation croisée pour évaluer la robustesse du modèle\n",
    "cv_scores = cross_val_score(rf_model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "print(f\"Validation croisée (Accuracy moyenne) : {np.mean(cv_scores):.4f}\")\n",
    "\n",
    "# Étape 3 : Optimisation des hyperparamètres avec GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=3, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Meilleurs paramètres trouvés\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Meilleurs hyperparamètres : {best_params}\")\n",
    "\n",
    "# Modèle optimisé\n",
    "rf_best_model = grid_search.best_estimator_\n",
    "\n",
    "# Étape 4 : Entraînement du modèle optimisé\n",
    "rf_best_model.fit(X_train, y_train)\n",
    "\n",
    "# Prédictions sur l'ensemble de test\n",
    "y_pred = rf_best_model.predict(X_test)\n",
    "y_pred_proba = rf_best_model.predict_proba(X_test)[:, 1]  # Probabilité pour la classe positive\n",
    "\n",
    "# Évaluer les performances\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(f\"Accuracy sur le test : {accuracy:.4f}\")\n",
    "print(f\"Rapport de classification :\\n{report}\")\n",
    "\n",
    "# Étape 5 : Courbe ROC et AUC\n",
    "if len(np.unique(y)) == 2:  # Vérification si problème binaire\n",
    "    auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "    print(f\"AUC-ROC Score : {auc_score:.4f}\")\n",
    "\n",
    "    # Tracer la courbe ROC\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='blue', label=f'ROC curve (AUC = {auc_score:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], color='red', linestyle='--')\n",
    "    plt.xlabel('Taux de faux positifs (FPR)')\n",
    "    plt.ylabel('Taux de vrais positifs (TPR)')\n",
    "    plt.title('Courbe ROC')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "# Étape 6 : Importance des caractéristiques\n",
    "feature_importances = rf_best_model.feature_importances_\n",
    "sorted_importances = sorted(zip(important_columns, feature_importances), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Importance des caractéristiques :\")\n",
    "for feature, importance in sorted_importances:\n",
    "    print(f\"{feature}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Supposons que 'SVHC' soit la variable cible\n",
    "X = df_train.drop(columns=['SVHC'])\n",
    "y = df_train['SVHC']\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Créer le modèle de Random Forest\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Entraîner le modèle\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Prédire sur l'ensemble de test\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Évaluer le modèle\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Classification Report:\\n{report}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
